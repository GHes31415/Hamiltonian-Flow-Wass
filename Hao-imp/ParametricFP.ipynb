{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19696,
     "status": "ok",
     "timestamp": 1646505631656,
     "user": {
      "displayName": "Hao Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11802573833201208899"
     },
     "user_tz": 300
    },
    "id": "0NPSN7eFYWRL",
    "outputId": "a044a8e5-e265-4a2a-8ca3-7499d1564983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1646505649878,
     "user": {
      "displayName": "Hao Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11802573833201208899"
     },
     "user_tz": 300
    },
    "id": "wsvm1KuaYXAC",
    "outputId": "b4bc46e4-26d8-422c-a517-a01040e7f3be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Parametric-Fokker-Planck-Equation-main\n"
     ]
    }
   ],
   "source": [
    "cd drive/My Drive/Parametric-Fokker-Planck-Equation-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 15443,
     "status": "ok",
     "timestamp": 1646505673325,
     "user": {
      "displayName": "Hao Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11802573833201208899"
     },
     "user_tz": 300
    },
    "id": "berskeVEYUWh"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import normalizing flows\n",
    "from flow import NormalizingFlow\n",
    "\n",
    "# import psi-network\n",
    "from psi_network import ReLUNN\n",
    "from psi_network import TanhNN\n",
    "\n",
    "# import loss functions\n",
    "from losses import Relative_Entropy\n",
    "from losses import JKO_loss\n",
    "from losses import Wass_loss\n",
    "from losses import JKO_loss_modified\n",
    "from losses import Wass_loss_modified\n",
    "\n",
    "# import potential functions\n",
    "from Quadratic_Function import Quadratic_Function_torch\n",
    "from Quadratic_Function2 import Quadratic_Function2_torch\n",
    "from Quadratic_Function_high_dimension import Quadratic_Function_high_dim_torch\n",
    "from Rosenbrock_Function import Rosenbrock_Function_torch\n",
    "from S_Tang_Function import S_Tang_Function_torch\n",
    "\n",
    "# import other utilities\n",
    "from utils import random_normal_samples\n",
    "from plot_sample import plot_sample\n",
    "from record_sample import record_sample_coord\n",
    "from plot_density import plot_density, plot_contour_density, plot_3d_density\n",
    "from utils import create_nodes, psi_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 157837,
     "status": "ok",
     "timestamp": 1646505892862,
     "user": {
      "displayName": "Hao Wu",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11802573833201208899"
     },
     "user_tz": 300
    },
    "id": "X31q5atPYHAX",
    "outputId": "c6ad6ba9-0e01-47e9-a5de-5b79e7ae1afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "finish one outer iteration\n",
      "Loss on iteration 1: -1.6568996906280518\n"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "#  Implementation of Algorithm 4.1 of \"NEURAL PARAMETRIC FOKKER-PLANCK EQUATION\"           #\n",
    "############################################################################################\n",
    "\n",
    "\n",
    "#########################\n",
    "# Setting Parameters    #\n",
    "#########################\n",
    "\n",
    "# select random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# dimension of the problem\n",
    "dimension = 2  # 10  # 30 \n",
    "\n",
    "# length of normalizing flow\n",
    "flowlength = 40  # for higher dimensions 10, 30, 50, please try 60, 60, 100\n",
    "\n",
    "# configuration of psi-network\n",
    "psi_network_length = 6  # for higher dimensions 10, 30, 50, please try 15, 30, 30\n",
    "hidden_dim = 20\n",
    "\n",
    "# time stepsize\n",
    "h = 0.005\n",
    "\n",
    "# number of time steps\n",
    "T = 1#00  # 600\n",
    "\n",
    "# sample size for evaluating relative entropy (KL-divergence)\n",
    "KL_sample_size = 12000\n",
    "\n",
    "# parameters for outer iterations\n",
    "theta_iterations = 30  # 50\n",
    "theta_batchsize = 5000\n",
    "theta_lr = 0.0005\n",
    "theta_opt_momentum = 0.9\n",
    "lrdecay = 0.999\n",
    "\n",
    "# set up epsilon (c.f. Remark 4.9), we may directly choose epsilon = theta_lr\n",
    "epsilon = 0.0005  # theta_lr\n",
    "\n",
    "# parameters for inner iterations\n",
    "psi_iterations = 60  # 100\n",
    "psi_batchsize = 1000\n",
    "psi_lr = 0.005\n",
    "psiopt_momentum = 0.8\n",
    "psi_lrdecay = 0.99\n",
    "\n",
    "# how often do we record data\n",
    "record_KL_period = 1  # record KL losses\n",
    "record_outer_loss_period = 10  # plot outer loss curve\n",
    "record_samples_period = 10  # plot samples and estimated densities\n",
    "record_psi_period = 15  # 50   # record inner loss curves and plot graphs of psi\n",
    "plot_num = 6000  # number of sample points to plot\n",
    "\n",
    "# plot on a-b-plane:\n",
    "a = 0 # 5\n",
    "b = 1 # 15\n",
    "# plotting range (on square [-L,L]x[-L,L])\n",
    "L = 7\n",
    "# nbins used for generate heat map that corresponds to the samples\n",
    "nbins = 600\n",
    "\n",
    "\n",
    "###################################################\n",
    "# Setting up functions and optimizers.            #\n",
    "###################################################\n",
    "\n",
    "# set up normalizing flow and an auxiliary flow\n",
    "flow = NormalizingFlow(dim=dimension, flow_length=flowlength)\n",
    "flow_auxil = NormalizingFlow(dim=dimension, flow_length=flowlength)  # auxiliary normalizing flow\n",
    "\n",
    "# set up psi-flow (Fully connected neural network with ReLU, Tanh or other kinds of activations)\n",
    "psi = ReLUNN(network_length=psi_network_length, hidden_dimension=hidden_dim, input_dimension=dimension, output_dimension=1)\n",
    "# psi = TanhNN(network_length=psi_network_length, hidden_dimension=hidden_dim, input_dimension=dimension, output_dimension=1)\n",
    "\n",
    "# define potential function\n",
    "potential = S_Tang_Function_torch\n",
    "# potential = Quadratic_Function_high_dim_torch\n",
    "# potential = Quadratic_Function_torch\n",
    "# potential = Quadratic_Function2_torch\n",
    "# potential = Rosenbrock_Function_torch\n",
    "\n",
    "# define loss functions\n",
    "relative_entropy_loss = Relative_Entropy(potential=potential, dimension=dimension)\n",
    "outer_loss = JKO_loss_modified(potential=potential, dimension=dimension, epsilon=epsilon, h=h)\n",
    "inner_loss = Wass_loss_modified(epsilon=epsilon)\n",
    "\n",
    "# define outer optimizer, we use ADAM here\n",
    "theta_optimizer = optim.Adam(flow.parameters(), lr=theta_lr, betas=(0.9, 0.999), eps=1e-08)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(theta_optimizer, lrdecay)\n",
    "\n",
    "# define inner optimizer, we use ADAM here\n",
    "psi_optimizer = optim.Adam(psi.parameters(), lr=psi_lr, betas=(0.9, 0.999), eps=1e-08)\n",
    "psi_scheduler = optim.lr_scheduler.ExponentialLR(psi_optimizer, psi_lrdecay)\n",
    "\n",
    "# Create a new folder\n",
    "folder = os.getcwd() + '/ Styblinski-Tang Potential DIM = {}'.format(dimension)\n",
    "os.makedirs(folder)\n",
    "\n",
    "# Set up path\n",
    "save_path = os.getcwd() + '/ Styblinski-Tang Potential DIM = {}'.format(dimension)\n",
    "\n",
    "# use to record KL losses along time\n",
    "record_KL_loss_list = []\n",
    "\n",
    "# Algorithm starts here\n",
    "for iter in range(1, T + 1):\n",
    "\n",
    "    # copy current parameters of T_\\theta to an auxiliary normalizing flow\n",
    "    flow_auxil.load_state_dict(flow.state_dict())\n",
    "\n",
    "    # compute KL loss (relative entropy) at this time step\n",
    "    samples = Variable(random_normal_samples(KL_sample_size, dim=dimension))\n",
    "    zk, log_jacobians = flow(samples)\n",
    "    KL_loss = relative_entropy_loss(zk, log_jacobians)\n",
    "\n",
    "    # initialize the parameters of psi\n",
    "    psi.initialization()\n",
    "\n",
    "    # print outer current time step\n",
    "    print(iter)\n",
    "\n",
    "    # use to record outer loss (loss_jko)\n",
    "    outer_loss_list = []\n",
    "\n",
    "    # outer iteration starts\n",
    "    for outer_iteration in range(1, theta_iterations + 1):\n",
    "\n",
    "        print(\"finish one outer iteration\")\n",
    "\n",
    "        inner_loss_list = []\n",
    "        # inner iteration starts\n",
    "        for inner_iteration in range(1, psi_iterations + 1):\n",
    "\n",
    "            # conduct inner optimization\n",
    "            # psi_scheduler.step()\n",
    "            samples = Variable(random_normal_samples(psi_batchsize, dim=dimension), requires_grad=True)\n",
    "            original_transformed_samples, _ = flow_auxil(samples)\n",
    "            v_original = Variable(original_transformed_samples, requires_grad=True)\n",
    "            current_transformed_samples, _ = flow(samples)\n",
    "            v_current = Variable(current_transformed_samples)\n",
    "            uk = psi(v_original)\n",
    "\n",
    "            gradients = torch.autograd.grad(outputs=uk, inputs=v_original, grad_outputs=torch.ones(uk.size()), create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "            psi_optimizer.zero_grad()\n",
    "            psi_loss = inner_loss(v_original, v_current, gradients)\n",
    "            psi_loss.backward()\n",
    "            psi_optimizer.step()\n",
    "\n",
    "            # record inner loss\n",
    "            inner_loss_list.append(psi_loss.item())\n",
    "\n",
    "        # conduct outer optimization\n",
    "        samples_1 = Variable(random_normal_samples(theta_batchsize, dim=dimension))\n",
    "        xxk, _ = flow_auxil(samples_1)\n",
    "        xk = Variable(xxk, requires_grad=True)\n",
    "        zk, log_jacobians = flow(samples_1)\n",
    "        psik = psi(xk)\n",
    "        gradients = \\\n",
    "        torch.autograd.grad(outputs=psik, inputs=xk, grad_outputs=torch.ones(psik.size()), create_graph=True,\n",
    "                            retain_graph=True, only_inputs=True)[0]\n",
    "        theta_optimizer.zero_grad()\n",
    "        theta_loss = outer_loss(zk, log_jacobians, gradients)\n",
    "        theta_loss.backward()\n",
    "        theta_optimizer.step()\n",
    "\n",
    "        # record the outer loss\n",
    "        outer_loss_list.append(theta_loss.item())\n",
    "\n",
    "        # record inner losses, plot inner loss curve and plot graphs of psi functions\n",
    "        if iter % record_psi_period == 0:\n",
    "\n",
    "            # write inner loss in a text file\n",
    "            filename_psilosslist = os.path.join(save_path, \"psi_loss: {}-th time step, {}-th outer iteration\".format(iter, outer_iteration) + \".txt\")\n",
    "            f = open(filename_psilosslist, \"w+\")\n",
    "            k = 0\n",
    "            for innerloss_data in inner_loss_list:\n",
    "                k = k + 1\n",
    "                f.write(\"{}. inner_loss: {} \\n\".format(k, innerloss_data))\n",
    "\n",
    "            # plot inner loss curve\n",
    "            num = psi_iterations\n",
    "            Nodes = create_nodes(num, 1)\n",
    "            Record_loss = torch.tensor(inner_loss_list)\n",
    "            plt.scatter(Nodes, Record_loss)\n",
    "            filename_psiloss = os.path.join(save_path, \"inner loss: {}-th time step, {}-th outer iteration\".format(iter, outer_iteration))\n",
    "            plt.savefig(filename_psiloss)\n",
    "            plt.close()\n",
    "\n",
    "            # plot the graphs of psi(x)\n",
    "            interval_width = 0.3\n",
    "            num_of_intervals = 30\n",
    "            Nodes = psi_nodes(interval_width, num_of_intervals, dimension)\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            NUMBER = (num_of_intervals + 1) * (num_of_intervals + 1)\n",
    "            for k in range(0, NUMBER):\n",
    "                x_node = Nodes[k][0]\n",
    "                y_node = Nodes[k][1]\n",
    "                graph_psi = psi(Nodes[k])\n",
    "                ax.scatter(x_node, y_node, graph_psi.detach().numpy())\n",
    "            filename_graph_psi = os.path.join(save_path, \"graph of psi(x) [{}th time step, {}th outer iteration] \".format(iter, outer_iteration))\n",
    "            plt.savefig(filename_graph_psi)\n",
    "            plt.close()\n",
    "\n",
    "    if iter % record_KL_period == 0:\n",
    "\n",
    "        print(\"Loss on iteration {}: {}\".format(iter, KL_loss.data.item()))\n",
    "        record_KL_loss_list.append(KL_loss.data.item())\n",
    "\n",
    "    if iter % record_samples_period == 0:\n",
    "\n",
    "        # plot samples\n",
    "        plot_samples = Variable(random_normal_samples(plot_num, dim=dimension))\n",
    "        zk, _ = flow(plot_samples)\n",
    "        plot_sample(a, b, zk.data.numpy(), iter, flowlength, L, save_path)\n",
    "\n",
    "        # record the coordinates of samples\n",
    "        record_sample_coord(zk.data.numpy(), iter, save_path)\n",
    "\n",
    "        # plot estimated density of the samples\n",
    "        plot_density(zk, a, b, nbins, iter, L, save_path)\n",
    "        plot_contour_density(zk, a, b, nbins, iter, L, save_path)\n",
    "        plot_3d_density(zk, a, b, nbins, iter, L, save_path)\n",
    "\n",
    "    if iter % record_outer_loss_period == 0:\n",
    "\n",
    "        # plot outer loss\n",
    "        num = theta_iterations\n",
    "        Nodes = create_nodes(num, 1)\n",
    "        Record_loss = torch.tensor(outer_loss_list)\n",
    "        plt.scatter(Nodes, Record_loss)\n",
    "        filename_outer_loss = os.path.join(save_path, \"outer loss at {}th time step\".format(iter))\n",
    "        plt.savefig(filename_outer_loss)\n",
    "        plt.close()\n",
    "\n",
    "# plot KL loss curve\n",
    "num = math.floor(iter/record_KL_period)\n",
    "Nodes = create_nodes(num, record_KL_period)\n",
    "Record_loss = torch.tensor(record_KL_loss_list)\n",
    "plt.scatter(Nodes, Record_loss)\n",
    "filename_Loss = os.path.join(save_path,  \"KL loss curve \")\n",
    "plt.savefig(filename_Loss)\n",
    "plt.close()\n",
    "\n",
    "# writing down parameters & data\n",
    "# write down parameters\n",
    "filename1 = os.path.join(save_path, \"parameters\"+\".txt\")\n",
    "f = open(filename1, \"w+\")\n",
    "f.write(\"dimension of the problem is %d \\n\" % dimension)\n",
    "f.write(\"\\n\")\n",
    "f.write(\"flowlength = %d \\n\" % flowlength)\n",
    "f.write(\"\\n\")\n",
    "f.write(\"number of time steps = %d \\n\" % T)\n",
    "f.write(\"stepsize h = %f \\n\" % h)\n",
    "f.write(\"sample size for evaluating relative entropy (KL-divergence) = %d \\n\" % KL_sample_size)\n",
    "f.write(\"outer batchsize = %d \\n\" % theta_batchsize)\n",
    "f.write(\"number of outer iterations = %d \\n\" % theta_iterations)\n",
    "f.write(\"outer learning rate = %f \\n\" % theta_lr)\n",
    "f.write(\"\\n\")\n",
    "f.write(\"inner batchsize = %d \\n\" % psi_batchsize)\n",
    "f.write(\"number of inner iterations = %d \\n\" % psi_iterations)\n",
    "f.write(\"inner learning rate = %f \\n\" % psi_lr)\n",
    "f.write(\"inner learning rate decay = %f \\n\" % psi_lrdecay)\n",
    "f.write(\"\\n\")\n",
    "f.write(\"period of recording KL losses = %d \\n\" % record_KL_period)\n",
    "f.write(\"period of recording samples = %d \\n\" % record_samples_period)\n",
    "f.write(\"number of samples used for plotting = %d \\n\" % plot_num)\n",
    "f.write(\"\\n\")\n",
    "f.write(\"plot coordinate a = %d \\n\" % a)\n",
    "f.write(\"plot coordinate b = %d \\n\" % b)\n",
    "f.write(\"plot range L = %f \\n\" % L)\n",
    "f.write(\"KDE nbins = %d \\n\" % nbins)\n",
    "f.write(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "# write down losses\n",
    "filename2 = os.path.join(save_path, \"KL loss data\"+\".txt\")\n",
    "f = open(filename2, \"w+\")\n",
    "k = 0\n",
    "for data in record_KL_loss_list:\n",
    "    k = k + 1\n",
    "    f.write(\"{}.loss:{} \\n\".format(k, data))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNenFfgc5uOGnc2FQEv+MxM",
   "name": "ParametricFP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
